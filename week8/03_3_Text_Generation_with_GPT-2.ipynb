{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c75fd9-75c4-4792-bf17-672a14193fef",
   "metadata": {},
   "source": [
    "# Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64aae2a-ac3c-4a59-8fbc-b680d9c1c8a3",
   "metadata": {},
   "source": [
    "**Objective:** To generate creative text based on a given prompt using GPT-2.\n",
    "\n",
    "We can use the Hugging Face's Transformers library, which is an open-source library for NLP tasks. They offer pre-trained models like GPT-2, which is a smaller and more accessible version of GPT-3. \n",
    "\n",
    "For this activity, you will see how a generative model can continue a given text in a meaningful way\n",
    "\n",
    "**Instructions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5c3c7-204f-4853-bbcc-ba6ed10ba568",
   "metadata": {},
   "source": [
    "## 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afe545-d6c8-4b05-8ac8-39310accdce0",
   "metadata": {},
   "source": [
    "- You may need to install the necessary libraries if you haven't already, You can install them via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579a39f6-5b2f-466a-a045-b2e3d4b7654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers\n",
    "#pip install torch #Requirement library for some functionalities of transformers library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbcabe5-9eb5-46ee-921f-036f15cccc13",
   "metadata": {},
   "source": [
    "- Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad52d880-e212-4e91-8d67-d78dd5a4c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd85f8-ccde-4a61-8eda-4379cef5624b",
   "metadata": {},
   "source": [
    "- We import `torch` which is the library for PyTorch, a popular framework for deep learning.\n",
    "- From `transformers`, we import `GPT2Tokenizer` and `GPT2LMHeadModel`. The tokenizer will help us convert text to numbers that the model can understand, and GPT2LMHeadModel is the actual GPT-2 model we'll be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115504d7-d96a-4c5e-8ffd-c694a245b98d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Load the pre-trained GPT-2 model and tokenizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470a24a-e20f-4333-9c61-931274376ecb",
   "metadata": {},
   "source": [
    "We load the pre-trained GPT-2 model and tokenizer using the from_pretrained method, specifying 'gpt2' as the model we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86fa7157-e80e-4d1a-b57a-30a15430c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882789f8-666c-4bef-a351-de0b6fb94f19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create a function to generate text based on a given prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8480db4-03d0-439a-8468-258a5196e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    # Encoding the inputs\n",
    "    inputs = tokenizer.encode(prompt,return_tensors=\"pt\")\n",
    "    \n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)  # Create an attention mask\n",
    "\n",
    "    # Generating outputs using the encoded inputs with simplified parameters\n",
    "    outputs = model.generate(\n",
    "        inputs, # Make sure to pass the encoded inputs\n",
    "        attention_mask=attention_mask,  # Pass the attention mask\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Set the pad token ID\n",
    "        max_length=150, # Output size\n",
    "        num_beams=5, # # Experiment with different values\n",
    "        temperature=0.9, # Lower temperature to make output more focused. Increase temperature for more randomness\n",
    "        do_sample=True,\n",
    "        top_k=50, # Lower top_k for more diversity\n",
    "        top_p=0.85, # Introduce top_p for nucleus sampling, for more diversity\n",
    "        no_repeat_ngram_size=2, # Prevent repeating n-grams of size 2. Increase to prevent repeating longer n-grams\n",
    "        early_stopping=True # Stop generating when conditions are met, to save time (in sometime Consider disabling)\n",
    "    )\n",
    "\n",
    "    # Decoding and printing the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f'Generated Text:\\n{generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c16e7-a7cb-4df0-8f9b-9c309f79c0f3",
   "metadata": {},
   "source": [
    "The function `generate_text` is designed to generate a piece of text based on a given prompt using a pre-trained language model.\n",
    "let's break it down:\n",
    "\n",
    "- **Input Encoding:**\n",
    "\n",
    " - `inputs = tokenizer.encode(prompt, return_tensors=\"pt\")`: \n",
    "\n",
    "      - The `prompt` is processed (tokenized) to convert it into a numerical format that the machine learning model can understand.\n",
    "   - The result is a tensor, which is a multi-dimensional array used in machine learning tasks, specifically formatted for PyTorch (indicated by return_tensors=\"pt\").\n",
    "   \n",
    "- **Text Generation:**\n",
    "\n",
    "    - `outputs = model.generate(inputs, max_length=150, num_beams=5, temperature=1.5, top_k=50)`:\n",
    "               - The pre-trained model is instructed to generate text based on the provided inputs.\n",
    "             - Various parameters like `max_length`, `num_beams`, `temperature`, `top_k`, ect are set to control the text generation process, impacting the length, creativity, and quality of the generated text.\n",
    "             \n",
    "\n",
    "- **Output Decoding:**\n",
    "\n",
    "  - `generated_text = tokenizer.decode(outputs[0])`:\n",
    "         - The numerical output from the model is translated back into human-readable text using the tokenizer.\n",
    "        - Only the first output (`outputs[0]`) is decoded as the model is set up to generate one piece of text in this case.\n",
    "\n",
    "- **Display Generated Text:**\n",
    "\n",
    "- `print(f'Generated Text:\\n{generated_text}')`:\n",
    "        - Finally, the generated text is printed to the console, prefixed with the label \"Generated Text:\".\n",
    "\n",
    "In summary, this function encapsulates the process of taking a textual prompt, processing it for the model, generating new text based on that prompt, decoding the generated text back into a human-readable form, and then displaying the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567dc0c-f3b5-4977-8ef5-33d712f5858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### About Text Generation Parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b8ce4-44f4-49ae-97c3-4bb2094927c4",
   "metadata": {},
   "source": [
    "Below are the parameters used in the model.generate method within the generate_text function:\n",
    "\n",
    "**max_length=150:**\n",
    "\n",
    "- This parameter sets the maximum number of tokens in the generated text. If the model reaches this length, it will stop generating further tokens.\n",
    "\n",
    "**num_beams=5:**\n",
    "\n",
    "- Beam search is a heuristic search algorithm used in machine learning. The `num_beams` parameter specifies the number of beams (or hypotheses) to maintain when generating text. A higher number of beams can result in better quality output, but at the cost of computational resources.\n",
    "\n",
    "**temperature=0.7:**\n",
    "\n",
    "- The `temperature` parameter helps control the randomness of the output. Lower values (like 0.7) make the output more focused and deterministic, while higher values make the output more random and creative.\n",
    "\n",
    "**top_k=50:**\n",
    "\n",
    "- During text generation, the `top_k` parameter restricts the selection pool for the next token to the top K probable tokens. This helps in reducing the chance of getting unlikely or rare tokens and keeps the generation process on track.\n",
    "\n",
    "**no_repeat_ngram_size=2:**\n",
    "\n",
    "- This parameter helps prevent the model from generating repeating n-grams (a sequence of n words) of size 2. This can aid in reducing repetitiveness in the generated text.\n",
    "\n",
    "**early_stopping=True:**\n",
    "\n",
    "- The `early_stopping` parameter is a boolean flag that, when set to `True`, stops the text generation process once certain conditions are met (like reaching an end-of-sequence token), helping to save time and computational resources.\n",
    "\n",
    "These parameters are used to control and fine-tune the text generation process, making it easier to obtain desirable and coherent text based on a given prompt.\n",
    "\n",
    "- **Note:**\n",
    "There are several other parameters we can include to control the text generation process using the `model.generate` method. The parameters and their descriptions can be found in the documentation for the specific library we are using in our case it is Hugging Face Transformers. Check out the following links:\n",
    "- [Hugging Face, OpenAI GPT2](https://huggingface.co/transformers/v2.11.0/model_doc/gpt2.html)\n",
    "- [Hugging Face, Model](https://huggingface.co/transformers/v2.11.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85650d24-a998-4bf7-8b18-2167f1f4d532",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Applying the generate_text function in out text Generated Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ada26c-4085-4d13-834d-f5228b2cdcc6",
   "metadata": {},
   "source": [
    "Now, call the generate_text function with a creative prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc1f74fe-d5ce-4689-9022-d66ee853e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Once upon a time, in a land far far away, there was a man who had been a prisoner of war. He was the son of a wealthy merchant, and his father was one of the richest men in the world.\n",
      "\n",
      "The man, who was in his early thirties, had a wife and two children. His father had died when he was twelve years old, but his mother was still alive when she was twenty-one. The man had lived with his family for many years. When he came to the United States, he found that he could not afford to pay for his education, so he went to a boarding school in New York, where he learned to read and write. There he received a scholarship to Harvard University\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "generate_text(\"Once upon a time, in a land far far away,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d06aa-3b68-4ee2-9e16-e0239ceaf5ff",
   "metadata": {},
   "source": [
    "## Activity Extension & Discussion: Enhancing Creativity in Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476804ca-8b85-4a35-af4e-1e03e2e8086b",
   "metadata": {},
   "source": [
    "Now that we have had our hands-on experience with basic text generation using GPT-2, let's dive a bit deeper to explore how we can tune the model to generate more creative text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19791f7-8ff2-4b97-aa3b-8edd8708e640",
   "metadata": {},
   "source": [
    "#### 1. Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f1ebe-7513-40be-a1a4-7003d2b94649",
   "metadata": {},
   "source": [
    "- **Temperature:**\n",
    "   - Adjusting the temperature parameter influences the randomness of the output.\n",
    "    - Higher values (e.g., closer to 1 or above) yield more creative and random outputs, whereas lower values make the output more focused and deterministic.\n",
    "- **Top_k:**\n",
    "    - The top_k parameter restricts the model to choose the next word from the top k probable words.\n",
    "     - Lower values of top_k can introduce more diversity in the generated text.\n",
    "- **Top_p (Nucleus Sampling):**\n",
    "     - The top_p parameter allows for a more dynamic truncation of the vocabulary during sampling.\n",
    "      - By introducing top_p, you're allowing the model to consider a varying set of most probable words to choose the next word from, which can lead to more creative text.\n",
    "- **Num_beams (Beam Search):**\n",
    "    - The num_beams parameter affects the beam search process, which tends to focus the output towards the most probable sequences.\n",
    "     - Adjusting or removing num_beams can increase creativity by letting the model explore a wider range of possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e45b2c-7c87-46fe-87e7-2d0192921784",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Activity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dcd1cc-d612-4b50-a397-0496e95b44fa",
   "metadata": {},
   "source": [
    "Update the generate_text function with new parameter values based on the above discussion.\n",
    "Compare the text generated with different parameter settings and discuss the observed changes in creativity and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f5ab860-8ed8-455b-80c5-a482afb21914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Once upon a time, in a land far far away, there was a man who did not know what he was talking about. He had been a member of the Order of St. Mary's.\n",
      "\n",
      "The young man had come to visit his father. When he had arrived, his mother had told him that her son was dead. The old man thought that he would not be able to find out what had happened to his son, but he did, and, as he said to himself, \"There is no one to tell me where my son is,\" and so he went back to the home of a friend. As he looked at the grave, he saw that it was covered with the remains of his brother, who had died a few years earlier. There was an inscription on it, which read: \"Father, my dear son. You are dead.\" He looked up and found that the inscription was not there. It was on a wall in the wall's center. \"It is not my place,\" he told the man. Then he remembered that there were three men there, all of whom he believed to have been killed. So he decided to go and look for him. But when he came upon them, they were very suspicious of him because they had seen him standing in front of their house. They asked him if he knew where the bodies were. At first he refused to answer, because he feared for his life. After that, however, the men in question began to\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt):\n",
    "    inputs = tokenizer.encode(prompt,return_tensors=\"pt\")\n",
    "    \n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)  # Create an attention mask\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs, # Make sure to pass the encoded inputs\n",
    "        attention_mask=attention_mask,  # Pass the attention mask\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Set the pad token ID\n",
    "        max_length=300, # Output size\n",
    "        num_beams=5, # Experiment with different values\n",
    "        temperature=1.5,  # Lower temperature to make output more focused. Increase temperature for more randomness\n",
    "        top_k=30,  # Lower top_k for more diversity\n",
    "        top_p=0.85,  # Introduce top_p for nucleus sampling, for more diversity\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=2,  # Prevent repeating n-grams of size 2. Increase to prevent repeating longer n-grams\n",
    "        early_stopping=True,  # Stop generating when conditions are met, to save time (in sometime Consider disabling)\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0])\n",
    "    print(f'Generated Text:\\n{generated_text}')\n",
    "\n",
    "# Example usage\n",
    "generate_text(\"Once upon a time, in a land far far away,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b786fe-d92e-47f9-b3da-aea59f1d4798",
   "metadata": {},
   "source": [
    "#### 3. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f7cfa-8ecd-4ed3-9246-da487157bd11",
   "metadata": {},
   "source": [
    "- How did the different parameter settings impact the creativity and coherence of the generated text?\n",
    "- Were there certain settings that produced more desirable or interesting results?\n",
    "- How might these parameter tweaks be useful in different text generation applications?\n",
    "\n",
    "This extension aims to provide a more nuanced understanding of how various parameters affect text generation with GPT-2, paving the way for attendees to experiment and discover optimal settings for their own use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e71789-9e3f-49d4-9e08-25c56cf725c2",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1e320-15e7-4b89-b5ce-3788f0faf6cb",
   "metadata": {},
   "source": [
    "- **Control**: Although we can influence the generated text with various parameters, achieving precise control over the content, style, or tone remains a challenge.\n",
    "- **Context Understanding**: The model might sometimes generate text that's contextually irrelevant or incorrect, as it solely relies on patterns learned from the training data without understanding the content.\n",
    "- **Lengthy Text**: Generating lengthy coherent text is still a challenge as the coherence often dwindles as the text gets longer.\n",
    "- **Computational Resources**: Generating text with large models like GPT-2 requires substantial computational resources, which might be a limitation for real-time applications or individuals with limited computational power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
